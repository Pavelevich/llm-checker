# LLM Checker ğŸš€

Advanced CLI tool that scans your hardware and tells you exactly **which LLM or sLLM models you can run locally**, with **full Ollama integration and intelligent cloud recommendations**.
![image](https://github.com/user-attachments/assets/77d156b7-e484-4475-80cc-2e4f275a161b)

---

## âœ¨ What's New in v2.1

- â˜ï¸ **Intelligent Ollama Cloud Search** â€“ Automatically finds compatible models from Ollama's library based on your specific hardware
- ğŸ¯ **Smart Model Filtering** â€“ Excludes already installed models and suggests only new, compatible options
- ğŸ“Š **Enhanced Compatibility Scoring** â€“ Advanced algorithm considers RAM ratios, model popularity, and hardware tiers
- ğŸ”„ **Structured Recommendations** â€“ Organized suggestions by category (installed, cloud, quick commands)
- ğŸ›¡ï¸ **Robust Fallback System** â€“ Curated model suggestions when cloud search is unavailable
- ğŸ¦™ **Full Ollama integration** â€“ Detects installed models, benchmarks performance and handles downloads automatically
- ğŸ£ **sLLM (Small Language Model) support** â€“ From 0.5 B all the way up to ultraâ€‘efficient models
- ğŸ“Š **Expanded model database** â€“ 40 + models including **Gemma 3, Phiâ€‘4, DeepSeekâ€‘R1, Qwen 2.5**
- ğŸ¯ **Improved compatibility analysis** â€“ Granular 0â€‘100 scoring system
- ğŸ·ï¸ **Detailed categorisation** â€“ ultraâ€‘small, small, medium, large, embedding, multimodal
- âš¡ **Performance estimation** â€“ tokens/s, memory footprint, energy consumption
- ğŸ§  **Useâ€‘caseâ€‘based recommendations** â€“ general, code, chat, embeddings, multimodal
- ğŸ“± **Redesigned CLI** â€“ cleaner UX with colours & emojis

---

## ğŸš€ Installation

### Option 1 â€“ Global NPM **(recommended)**

```bash
npm install -g llm-checker
```

### Option 2 â€“ With Ollama **(recommended for running models)**

```bash
# 1 Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2 Install LLM Checker
npm install -g llm-checker

# 3 Verify
llm-checker check
```

### Option 3 â€“ Local development

```bash
git clone https://github.com/developer31f/llm-checker.git
cd llm-checker
npm install
npm link
```

---

## ğŸ“– Usage

### Main command â€“ full analysis

```bash
# Full system scan + Ollama detection + cloud recommendations
llm-checker check

# Detailed hardware info
llm-checker check --detailed

# Run a performance benchmark
llm-checker check --performance-test
```

#### Filter by model category

```bash
llm-checker check --filter ultra_small   # Models < 1 B params
llm-checker check --filter small         # 1â€“4 B
llm-checker check --filter medium        # 5â€“15 B
llm-checker check --filter large         # > 15 B
```

#### Filter by specialisation

```bash
llm-checker check --filter code         # Programming models
llm-checker check --filter chat         # Conversational
llm-checker check --filter multimodal   # Vision + text
llm-checker check --filter embeddings   # Embedding models
```

#### Useâ€‘case presets

```bash
llm-checker check --use-case code        # Optimised for coding
llm-checker check --use-case chat        # Optimised for conversation
llm-checker check --use-case embeddings  # Semantic search
```

#### Ollamaâ€‘only / include cloud

```bash
llm-checker check --ollama-only          # Only local models
llm-checker check --include-cloud        # Include cloud models for comparison
```

---

### Ollama management

```bash
# List installed models
llm-checker ollama --list

# Show running models
llm-checker ollama --running

# Benchmark a specific model
llm-checker ollama --test llama3.2:3b

# Download a model
llm-checker ollama --pull phi3:mini

# Remove a model
llm-checker ollama --remove old-model:tag
```

---

### Explore the model database

```bash
llm-checker browse                 # All models
llm-checker browse --category small
llm-checker browse --year 2024
llm-checker browse --multimodal
```

---

### Help commands

```bash
llm-checker --help                # Global help
llm-checker check --help          # Help for a specific command
llm-checker ollama --help
```

---

## ğŸ¯ Example output

### Highâ€‘end system with Ollama and Cloud Recommendations

```
ğŸ–¥ï¸  System Information:
CPU: Apple M2 Pro (12 cores, 3.5 GHz)
Architecture: Apple Silicon
RAM: 32 GB total (24 GB free, 25 % used)
GPU: Apple M2 Pro (16 GB VRAM, dedicated)
OS: macOS Sonoma 14.2.1 (arm64)

ğŸ† Hardware Tier: HIGH (Overall Score: 92/100)

ğŸ¦™ Ollama Status: âœ… Running (v0.1.17)
ğŸ“¦ Local Models: 5 installed, 3 compatible
ğŸš€ Running Models: llama3.1:8b

âš¡ Performance Benchmark:
CPU Score: 95/100
Memory Score: 88/100
Overall Score: 91/100

âœ… Compatible Models (Score â‰¥ 75):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model               â”‚ Size     â”‚ Score     â”‚ RAM      â”‚ VRAM     â”‚ Speed     â”‚ Status   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Llama 3.1 8B ğŸ¦™     â”‚ 8 B      â”‚ 98/100    â”‚ 8 GB     â”‚ 4 GB     â”‚ medium    â”‚ ğŸš€ Runningâ”‚
â”‚ Mistral 7B v0.3 ğŸ¦™  â”‚ 7 B      â”‚ 97/100    â”‚ 8 GB     â”‚ 4 GB     â”‚ medium    â”‚ ğŸ“¦ Installedâ”‚
â”‚ CodeLlama 7B ğŸ¦™     â”‚ 7 B      â”‚ 97/100    â”‚ 8 GB     â”‚ 4 GB     â”‚ medium    â”‚ ğŸ“¦ Installedâ”‚
â”‚ Phiâ€‘3 Mini 3.8B     â”‚ 3.8 B    â”‚ 99/100    â”‚ 4 GB     â”‚ 2 GB     â”‚ fast      â”‚          â”‚
â”‚ Gemma 3 1B          â”‚ 1 B      â”‚ 100/100   â”‚ 2 GB     â”‚ 0 GB     â”‚ very_fast â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ Smart Recommendations:
ğŸ’¡ General Recommendations:
   1. ğŸš€ Any model size supported - try the largest available
   2. ğŸ’ Use Q6_K or Q8_0 for maximum quality
   3. ğŸ¦™ Install Ollama for easy model management

ğŸ“¦ Your Installed Ollama Models:
   ğŸ¦™ 3 compatible models found in Ollama:
   1. ğŸ“¦ llama3.1:8b (Score: 98/100) ğŸš€ (running)
   2. ğŸ“¦ mistral:7b (Score: 97/100)
   3. ğŸ“¦ codellama:7b (Score: 97/100)

â˜ï¸ Recommended from Ollama Cloud:
   ğŸ’¡ Recommended models from Ollama library for your hardware:
   1. ğŸš€ ollama pull deepseek-r1:7b - Advanced reasoning model, state-of-the-art (2,547,832 pulls)
   2. ğŸš€ ollama pull qwen2.5:14b - Large Chinese model with excellent capabilities (1,892,156 pulls)
   3. ğŸš€ ollama pull gemma2:27b - Google's flagship model for your tier (987,234 pulls)
   4. ğŸš€ ollama pull phi4:14b - Microsoft's latest model with improved reasoning (756,891 pulls)

âš¡ Quick Commands:
   > ollama run llama3.1:8b
   > ollama pull deepseek-r1:7b
   > ollama pull qwen2.5:14b

ğŸ¯ Next Steps:
1. ğŸš€ Install a recommended model from above
2. ğŸ’¬ Start chatting: ollama run <model-name>
3. ğŸ“Š Analyze: llm-checker analyze-model <model>
```

### Resourceâ€‘limited system with Cloud Suggestions

```
ğŸ–¥ï¸  System Information:
CPU: Intel Core i5â€‘8400 (6 cores, 2.8 GHz)
Architecture: x86â€‘64
RAM: 8 GB total (3 GB free, 62 % used)
GPU: Intel UHD Graphics 630 (0 GB VRAM, integrated)
OS: Ubuntu 22.04 LTS (x64)

ğŸ† Hardware Tier: LOW (Overall Score: 45/100)

ğŸ¦™ Ollama Status: âœ… Running (v0.1.17)
ğŸ“¦ No models installed yet

ğŸ¯ Smart Recommendations:
ğŸ’¡ General Recommendations:
   1. ğŸ¤ Small models (1B-3B) work well on your system
   2. ğŸ¯ Use Q4_0 quantization for good balance
   3. ğŸ¦™ Install Ollama for easy model management

â˜ï¸ Recommended from Ollama Cloud:
   ğŸ’¡ Recommended models from Ollama library for your hardware:
   1. ğŸš€ ollama pull qwen:0.5b - Ultra-efficient model, great for limited hardware (3,245,123 pulls)
   2. ğŸš€ ollama pull tinyllama:1.1b - Tiny but capable, perfect for testing (2,891,456 pulls)
   3. ğŸš€ ollama pull phi3:mini - Microsoft's efficient model with excellent reasoning (1,934,567 pulls)
   4. ğŸš€ ollama pull llama3.2:1b - Meta's latest compact model (1,567,890 pulls)

âš¡ Quick Commands:
   > ollama pull qwen:0.5b
   > ollama pull tinyllama:1.1b
   > ollama pull phi3:mini

ğŸ¯ Next Steps:
1. ğŸš€ Install a recommended model from above
2. ğŸ’¬ Start chatting: ollama run <model-name>
```

---

## â˜ï¸ Intelligent Cloud Recommendations

LLM Checker now features an **intelligent cloud search system** that automatically finds the best models for your specific hardware from Ollama's extensive library.

### How it works

1. **Hardware Analysis**: Evaluates your RAM, CPU cores, and architecture
2. **Smart Filtering**: Excludes models you already have installed
3. **Compatibility Scoring**: Rates each model based on:
    - RAM requirements vs available memory
    - Model size optimization for your tier
    - CPU compatibility and core count
    - Model popularity and reliability
    - Architecture-specific optimizations

4. **Curated Suggestions**: Returns the top 5 most compatible models
5. **Fallback System**: Provides curated suggestions if cloud search fails

### Cloud recommendation scoring factors

- **RAM Compatibility** (40%): Model memory requirements vs available RAM
- **Model Size Optimization** (25%): Preference for appropriately sized models
- **Hardware Tier Matching** (15%): Bonus for models suited to your hardware class
- **Popularity & Reliability** (10%): Models with high download counts
- **Architecture Bonuses** (10%): Apple Silicon, x86-64, ARM optimizations

### Supported hardware tiers

- ğŸš€ **ULTRA_HIGH** (64GB+ RAM): All models, including 70B+ parameters
- âš¡ **HIGH** (32GB+ RAM): Large models up to 34B parameters
- ğŸ¯ **MEDIUM** (16GB+ RAM): Medium models 7B-14B parameters
- ğŸ’» **LOW** (8GB+ RAM): Small models 1B-7B parameters
- ğŸ“± **ULTRA_LOW** (<8GB RAM): Ultra-small models 0.5B-3B parameters

---

## ğŸ”§ Supported models (40 +)

### ğŸ£ Ultraâ€‘small (< 1 B params)

- **Qwen 0.5B** â€“ Ultra lightweight, requires 1 GB RAM
- **LaMiniâ€‘GPT 774 M** â€“ Multilingual compact model, 1.5 GB RAM

### ğŸ¤ Small (1 â€“ 4 B)

- **TinyLlama 1.1 B** â€“ Perfect for testing, 2 GB RAM
- **Gemma 3 1 B** â€“ Mobileâ€‘optimised, 2 GB RAM, 32 K context
- **MobileLLaMA 1.4 B / 2.7 B** â€“ 40 % faster than TinyLlama
- **Llama 3.2 1 B / 3 B** â€“ Compact Meta models
- **Phiâ€‘3 Mini 3.8 B** â€“ Great reasoning from Microsoft, 4 GB RAM
- **Gemma 2 B** â€“ Efficient Google model, 3 GB RAM

### ğŸ¦ Medium (5 â€“ 15 B)

- **Llama 3.1 8 B** â€“ Perfect balance, 8 GB RAM
- **Mistral 7 B v0.3** â€“ Highâ€‘quality EU model, 8 GB RAM
- **Qwen 2.5 7 B** â€“ Multilingual with strong coding ability
- **CodeLlama 7 B** â€“ Specialised for coding, 8 GB RAM
- **DeepSeek Coder 6.7 B** â€“ Advanced code generation
- **Phiâ€‘4 14 B** â€“ Latest Microsoft model with improved capabilities
- **Gemma 3 4 B** â€“ Multimodal with long context (128 K)

### ğŸ¦… Large (> 15 B)

- **Llama 3.3 70 B** â€“ Meta flagship, 48 GB RAM
- **DeepSeekâ€‘R1 70 B** â€“ Advanced reasoning (o1â€‘style)
- **Mistral Small 3.1 22 B** â€“ Highâ€‘end EU model
- **Gemma 3 12 B / 27 B** â€“ Google multimodal flagships
- **CodeLlama 34 B** â€“ Heavy coding tasks, 24 GB RAM
- **Mixtral 8Ã—7 B** â€“ Mixtureâ€‘ofâ€‘Experts, 32 GB RAM

### ğŸ–¼ï¸ Multimodal (Vision + text)

- **LLaVA 7 B** â€“ Image understanding, 10 GB RAM
- **LLaVAâ€‘NeXT 34 B** â€“ Advanced vision capabilities
- **Gemma 3 4 B / 12 B / 27 B** â€“ Google multimodal family

### ğŸ§² Embedding models (semantic search)

- **allâ€‘MiniLMâ€‘L6â€‘v2** â€“ Compact 0.5 GB embedding model
- **BGEâ€‘smallâ€‘enâ€‘v1.5** â€“ Highâ€‘quality English embeddings

### â˜ï¸ Cloud models (for comparison)

- **GPTâ€‘4** â€“ OpenAI, requires API key & internet
- **Claude 3.5 Sonnet** â€“ Anthropic, 200 K context

---

## ğŸ› ï¸ Advanced Ollama integration

### Automatic model management

```bash
llm-checker ollama --list       # Details of every local model
llm-checker ollama --running    # Monitor VRAM usage
llm-checker ollama --test llama3.1:8b
```

### Smart installation with cloud recommendations

```bash
# Get personalized recommendations for your hardware
llm-checker check

# Install all recommended cloud models automatically
llm-checker check | grep "ollama pull" | head -3 | bash

# Install specific model from cloud recommendations
ollama pull qwen:0.5b  # Ultra-efficient for limited hardware
ollama pull phi3:mini  # Great reasoning for medium systems
```

### Realâ€‘time model comparison

```bash
for model in $(ollama list | grep -v NAME | awk '{print $1}'); do
  echo "Testing $model:"
  llm-checker ollama --test $model
done
```

### Debug cloud recommendations

```bash
# Enable debug logging to see cloud search process
DEBUG=1 llm-checker check

# Test cloud search specifically
export LLM_CHECKER_LOG_LEVEL=debug
llm-checker check --detailed
```

---

## ğŸ“Š Detailed compatibility system

### Scoring scale (0â€‘100)

| Score | Meaning      |
|-------|--------------|
| 90â€‘100 | ğŸŸ¢ **Excellent** â€“ full speed, all features |
| 75â€‘89  | ğŸŸ¡ **Very good** â€“ great performance |
| 60â€‘74  | ğŸŸ  **Marginal** â€“ usable with tweaks / quantisation |
| 40â€‘59  | ğŸ”´ **Limited** â€“ only for testing |
| 0â€‘39   | âš« **Incompatible** â€“ missing critical hw |

### Compatibility factors

1. **Total RAM vs requirement** (40 %)
2. **Available VRAM** (25 %)
3. **CPU cores** (15 %)
4. **CPU architecture** (10 %)
5. **Quantisation availability** (10 %)

### Cloud recommendation factors

1. **RAM ratio** (40%): Available RAM vs model requirements
2. **Model size optimization** (25%): Appropriate size for hardware tier
3. **Hardware tier bonuses** (15%): Tier-specific optimizations
4. **CPU compatibility** (10%): Core count and architecture matching
5. **Popularity & quality** (10%): Download counts and official status

### Hardware tiers

- ğŸš€ **ULTRA_HIGH** â€“ 64 GB RAM, 32 GB VRAM, 12+ cores
- âš¡ **HIGH** â€“ 32 GB RAM, 16 GB VRAM, 8+ cores
- ğŸ¯ **MEDIUM** â€“ 16 GB RAM, 8 GB VRAM, 6+ cores
- ğŸ’» **LOW** â€“ 8 GB RAM, 2 GB VRAM, 4+ cores
- ğŸ“± **ULTRA_LOW** â€“ below the above

---

## âš™ï¸ Advanced configuration

### Environment variables

```bash
export OLLAMA_BASE_URL=http://localhost:11434
export LLM_CHECKER_RAM_GB=16
export LLM_CHECKER_VRAM_GB=8
export LLM_CHECKER_CPU_CORES=8
export LLM_CHECKER_LOG_LEVEL=debug
export LLM_CHECKER_CACHE_DIR=/custom/cache
export LLM_CHECKER_CLOUD_SEARCH=true
export NO_COLOR=1
```

### Configuration file `~/.llm-checker.json`

```json
{
  "analysis": {
    "defaultUseCase": "code",
    "performanceTesting": true,
    "includeCloudModels": false,
    "enableCloudSearch": true,
    "maxCloudSuggestions": 5
  },
  "ollama": {
    "baseURL": "http://localhost:11434",
    "enabled": true,
    "timeout": 30000,
    "cloudSearchEnabled": true
  },
  "display": {
    "maxModelsPerTable": 15,
    "showEmojis": true,
    "compactMode": false,
    "showCloudRecommendations": true
  },
  "filters": {
    "minCompatibilityScore": 70,
    "excludeModels": ["very-large-model"],
    "minCloudScore": 60
  },
  "customModels": [
    {
      "name": "Custom Model 7B",
      "size": "7B",
      "requirements": { "ram": 8, "vram": 4 }
    }
  ]
}
```

---

## ğŸ® Specific use cases

### Coding

```bash
llm-checker check --use-case code --filter medium
# Look for cloud recommendations for coding models
ollama pull $(llm-checker check --use-case code | grep "codellama\|deepseek\|phi" | head -1 | awk '{print $3}')
echo "def fibonacci(n):" | ollama run codellama:7b "Complete this Python function"
```

### Chatbots / assistants

```bash
llm-checker check --use-case chat --filter small,medium
# Install recommended chat model from cloud
ollama pull llama3.2:3b
ollama run llama3.2:3b "Hello! How can you help me today?"
```

### Semantic search / RAG

```bash
llm-checker check --filter embeddings
ollama pull all-minilm
```

### Multimodal analysis

```bash
llm-checker check --multimodal
ollama pull llava:7b
```

### Hardware-optimized workflow

```bash
# Get recommendations for your specific hardware
llm-checker check > recommendations.txt

# Install the top 3 recommended models
grep "ollama pull" recommendations.txt | head -3 | bash

# Test performance of installed models
ollama list | grep -v NAME | awk '{print $1}' | xargs -I {} llm-checker ollama --test {}
```

---

## ğŸ” Troubleshooting

### Ollama not detected

```bash
curl http://localhost:11434/api/version          # Should return JSON

# Install or start service
curl -fsSL https://ollama.ai/install.sh | sh
sudo systemctl start ollama                      # Linux
```

### Cloud recommendations not showing

```bash
# Check Ollama cloud connectivity
curl -s https://ollama.ai/api/tags | head -20

# Enable debug mode
export LLM_CHECKER_LOG_LEVEL=debug
llm-checker check

# Force cloud search
export LLM_CHECKER_CLOUD_SEARCH=true
llm-checker check --detailed
```

### Incorrect hardware detection

```bash
llm-checker check --detailed
export LLM_CHECKER_RAM_GB=16
export LLM_CHECKER_VRAM_GB=8
llm-checker check
```

### Models marked as incompatible

```bash
llm-checker check --min-score 0
llm-checker check --include-marginal
llm-checker check --quantization Q2_K
```

### Permission errors (Linux/macOS)

```bash
sudo chmod +r /sys/class/dmi/id/*
sudo chmod +r /proc/meminfo
sudo llm-checker check
```

---

## ğŸ§ª Programmatic API (Node)

```javascript
const LLMChecker = require('llm-checker');
const OllamaClient = require('llm-checker/ollama');

const checker = new LLMChecker();
const analysis = await checker.analyze({
  useCase: 'code',
  includeCloud: false,
  performanceTest: true
});

console.log('Compatible models:', analysis.compatible);
console.log('Cloud recommendations:', analysis.recommendations.cloudSuggestions);

// Get cloud recommendations directly
const hardware = await checker.getSystemInfo();
const installedModels = await checker.getOllamaInfo();
const cloudRecs = await checker.searchOllamaCloudRecommendations(hardware, installedModels.compatibleOllamaModels);

const ollama = new OllamaClient();
const localModels = await ollama.getLocalModels();
```

---

## ğŸš€ CI/CD Workflows

### GitHub Actions

```yaml
name: LLM Compatibility Check
on: [push, pull_request]

jobs:
  llm-check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'

    - name: Install LLM Checker
      run: npm install -g llm-checker

    - name: Check LLM Compatibility
      run: |
        llm-checker check --format json > compatibility.json
        cat compatibility.json

    - name: Upload Results
      uses: actions/upload-artifact@v4
      with:
        name: llm-compatibility
        path: compatibility.json
```

### Docker integration

```dockerfile
FROM node:18-alpine

RUN apk add --no-cache dmidecode lm-sensors curl
RUN npm install -g llm-checker

# Install Ollama for cloud recommendations
RUN curl -fsSL https://ollama.ai/install.sh | sh

COPY analyze.sh /usr/local/bin/
RUN chmod +x /usr/local/bin/analyze.sh

CMD ["analyze.sh"]
```

---

## ğŸ¤ Contributing

### Local development

```bash
git clone https://github.com/developer31f/llm-checker.git
cd llm-checker
npm install
npm test
npm run dev check
npm link
```

### Adding new models

Edit `src/models/expanded_database.js`:

```javascript
{
  name: "New Model 7B",
  size: "7B",
  type: "local",
  category: "medium",
  requirements: { ram: 8, vram: 4, cpu_cores: 4, storage: 7 },
  frameworks: ["ollama", "llama.cpp"],
  quantization: ["Q4_0", "Q4_K_M", "Q5_0"],
  performance: {
    speed: "medium",
    quality: "very_good",
    context_length: 8192,
    tokens_per_second_estimate: "15-30"
  },
  installation: {
    ollama: "ollama pull new-model:7b",
    description: "Description of the new model"
  },
  specialization: "general",
  languages: ["en"],
  year: 2024
}
```

### Improving cloud search

Edit `src/index.js` to enhance the `searchOllamaCloudRecommendations` method:

```javascript
calculateCloudModelCompatibility(model, hardware) {
  // Add new scoring factors
  let score = 50;
  
  // Your custom compatibility logic
  if (model.specialization === 'custom') {
    score += 10;
  }
  
  return Math.max(0, Math.min(100, score));
}
```

### Improving hardware detection

Edit `src/hardware/detector.js`:

```javascript
detectNewGPU(gpu) {
  const model = gpu.model.toLowerCase();
  if (model.includes('new-gpu')) {
    return {
      dedicated: true,
      performance: 'high',
      optimizations: ['new-api']
    };
  }
  return null;
}
```

### Contribution guide

1. **Fork** the repository
2. **Create** a feature branch `git checkout -b feature/awesome-feature`
3. **Commit** your changes `git commit -am 'Add awesome feature'`
4. **Push** the branch `git push origin feature/awesome-feature`
5. **Open** a Pull Request

### Reporting issues

Include the following:

```bash
llm-checker check --detailed --export json > debug-info.json
DEBUG=1 llm-checker check > debug.log 2>&1
uname -a
node --version
npm --version
```

---

## ğŸ“Š Roadmap

### v2.2 (Q2 2025)

- ğŸ”Œ Plugin system for extensions
- ğŸŒ Enhanced cloud model search with filters
- ğŸ“Š Model performance prediction based on real usage data
- ğŸ¯ Custom model compatibility profiles
- ğŸ“ˆ Usage metrics & analytics

### v2.3 (Q3 2025)

- ğŸ¤– More backâ€‘end frameworks (MLX, TensorRT)
- â˜ï¸ Multi-provider cloud comparison (Ollama, HuggingFace, OpenRouter)
- ğŸ¯ Fineâ€‘tuning advisor with hardware requirements
- ğŸ“Š Historical performance dashboard
- ğŸ”’ Enterprise mode with team sharing

### v3.0 (Q4 2025)

- ğŸ§  AI performance predictor using machine learning
- ğŸ”„ Multiâ€‘model orchestration for different tasks
- ğŸ“± Mobile companion app
- ğŸŒ Advanced multilingual model recommendations
- ğŸš€ Public cloud integrations (AWS, GCP, Azure)

---

## ğŸ† Credits

### Technologies used

- **systeminformation** â€“ crossâ€‘platform HW detection
- **Ollama** â€“ local LLM management and cloud search
- **Commander.js** â€“ CLI framework
- **Chalk** â€“ terminal colours
- **Ora** â€“ elegant spinners

### Communities & projects

- **llama.cpp** â€“ efficient LLM inference
- **Hugging Face** â€“ model & dataset hub
- **Meta Llama** â€“ openâ€‘source Llama models
- **Mistral AI** â€“ European LLMs
- **Google Gemma** â€“ open Gemma family
- **Ollama Community** â€“ for the extensive model library

### Contributors

- **[Pavel Chmirenko](mailto:developer31f@gmail.com)** â€“ Lead developer & maintainer

### Special thanks

- **The Ollama team** for an amazing local LLM tool and extensive model library
- **Georgi Gerganov** for `llama.cpp`
- **The openâ€‘source community** for making AI accessible
- **Model creators** who make their models available through Ollama

---

## ğŸ“„ License

MIT â€“ see [LICENSE](LICENSE) for details.

---

## ğŸ’ Support the project

If **LLM Checker** saves you time finding the perfect models for your hardware:

â­ **Star** the repo  
ğŸ› **Report bugs** & suggest features  
ğŸ¤ **Contribute** code or docs  
ğŸ“¢ **Share** with fellow devs  
â˜• **[Buy me a coffee](https://buymeacoffee.com/pavelchmirenko)**

---

<div align="center">

**Got questions?** ğŸ’¬ [Open an issue](https://github.com/developer31f/llm-checker/issues)  
**Want to contribute?** ğŸš€ [Read the guide](CONTRIBUTING.md)  
**Need advanced usage?** ğŸ“š [Full docs](ADVANCED_USAGE.md)

**Made with â¤ï¸ for the openâ€‘source AI community**

</div>