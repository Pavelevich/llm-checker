# LLM CheckerÂ 2.0.3Â ğŸš€

Advanced CLI tool that scans your hardware and tells you exactly **which LLM or sLLM models you can run locally**, with **full Ollama integration**.
![image](https://github.com/user-attachments/assets/77d156b7-e484-4475-80cc-2e4f275a161b)


---

## âœ¨ Whatâ€™s New inÂ v2.0

- ğŸ¦™ **Full Ollama integration** â€“ Detects installed models, benchmarks performance and handles downloads automatically
- ğŸ£ **sLLM (Small Language Model) support** â€“ From 0.5â€¯B all the way up to ultraâ€‘efficient models
- ğŸ“Š **Expanded model database** â€“ 40â€¯+ models including **GemmaÂ 3,Â Phiâ€‘4,Â DeepSeekâ€‘R1,Â QwenÂ 2.5**
- ğŸ¯ **Improved compatibility analysis** â€“ Granular 0â€‘100 scoring system
- ğŸ·ï¸ **Detailed categorisation** â€“ ultraâ€‘small, small, medium, large, embedding, multimodal
- âš¡ **Performance estimation** â€“ tokens/s, memory footprint, energy consumption
- ğŸ§  **Useâ€‘caseâ€‘based recommendations** â€“ general, code, chat, embeddings, multimodal
- ğŸ“± **Redesigned CLI** â€“ cleaner UX with colours & emojis

---

## ğŸš€ Installation

### OptionÂ 1Â â€“ GlobalÂ NPM **(recommended)**

```bash
npm install -g llm-checker
```

### OptionÂ 2Â â€“ WithÂ Ollama **(recommended for running models)**

```bash
# 1Â Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2Â Install LLMÂ Checker
npm install -g llm-checker

# 3Â Verify
llm-checker check
```

### OptionÂ 3Â â€“ Local development

```bash
git clone https://github.com/developer31f/llm-checker.git
cd llm-checker
npm install
npm link
```

---

## ğŸ“– Usage

### Main command â€“ full analysis

```bash
# Full system scan + Ollama detection
llm-checker check

# Detailed hardware info
llm-checker check --detailed

# Run a performance benchmark
llm-checker check --performance-test
```

#### Filter by model category

```bash
llm-checker check --filter ultra_small   # Models <Â 1â€¯B params
llm-checker check --filter small         # 1â€“4â€¯B
llm-checker check --filter medium        # 5â€“15â€¯B
llm-checker check --filter large         # >â€¯15â€¯B
```

#### Filter by specialisation

```bash
llm-checker check --filter code         # Programming models
llm-checker check --filter chat         # Conversational
llm-checker check --filter multimodal   # VisionÂ +Â text
llm-checker check --filter embeddings   # Embedding models
```

#### Useâ€‘case presets

```bash
llm-checker check --use-case code        # Optimised for coding
llm-checker check --use-case chat        # Optimised for conversation
llm-checker check --use-case embeddings  # Semantic search
```

#### Ollamaâ€‘only / include cloud

```bash
llm-checker check --ollama-only          # Only local models
llm-checker check --include-cloud        # Include cloud models for comparison
```

---

### Ollama management

```bash
# List installed models
llm-checker ollama --list

# Show running models
llm-checker ollama --running

# Benchmark a specific model
llm-checker ollama --test llama3.2:3b

# Download a model
llm-checker ollama --pull phi3:mini

# Remove a model
llm-checker ollama --remove old-model:tag
```

---

### Explore the model database

```bash
llm-checker browse                 # All models
llm-checker browse --category small
llm-checker browse --year 2024
llm-checker browse --multimodal
```

---

### Help commands

```bash
llm-checker --help                # Global help
llm-checker check --help          # Help for a specific command
llm-checker ollama --help
```

---

## ğŸ¯ Example output

### Highâ€‘end system with Ollama

```
ğŸ–¥ï¸  System Information:
CPU: AppleÂ M2Â ProÂ (12Â cores,Â 3.5â€¯GHz)
Architecture: AppleÂ Silicon
RAM: 32â€¯GB total (24â€¯GB free, 25â€¯% used)
GPU: AppleÂ M2Â ProÂ (16â€¯GB VRAM, dedicated)
OS: macOSâ€¯Sonomaâ€¯14.2.1Â (arm64)

ğŸ† Hardware Tier: HIGHÂ (OverallÂ Score:Â 92/100)

ğŸ¦™ Ollama Status: âœ… RunningÂ (v0.1.17)
ğŸ“¦ Local Models: 5 installed
ğŸš€ Running Models: llama3.1:8b

âš¡ Performance Benchmark:
CPU Score: 95/100
Memory Score: 88/100
Overall Score: 91/100

âœ… Compatible Models (ScoreÂ â‰¥Â 75):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model               â”‚ Size     â”‚ Score     â”‚ RAM      â”‚ VRAM     â”‚ Speed     â”‚ Status   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LlamaÂ 3.1Â 8B        â”‚ 8â€¯B      â”‚ 98/100    â”‚ 8â€¯GB     â”‚ 4â€¯GB     â”‚ medium    â”‚ ğŸ“¦ ğŸš€    â”‚
â”‚ MistralÂ 7BÂ v0.3     â”‚ 7â€¯B      â”‚ 97/100    â”‚ 8â€¯GB     â”‚ 4â€¯GB     â”‚ medium    â”‚ ğŸ“¦       â”‚
â”‚ CodeLlamaÂ 7B        â”‚ 7â€¯B      â”‚ 97/100    â”‚ 8â€¯GB     â”‚ 4â€¯GB     â”‚ medium    â”‚ ğŸ’»       â”‚
â”‚ Phiâ€‘3Â MiniÂ 3.8B     â”‚ 3.8â€¯B    â”‚ 99/100    â”‚ 4â€¯GB     â”‚ 2â€¯GB     â”‚ fast      â”‚          â”‚
â”‚ GemmaÂ 3Â 1B          â”‚ 1â€¯B      â”‚ 100/100   â”‚ 2â€¯GB     â”‚ 0â€¯GB     â”‚ very_fast â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Resourceâ€‘limited system

```
ğŸ–¥ï¸  System Information:
CPU: IntelÂ CoreÂ i5â€‘8400Â (6Â cores,Â 2.8â€¯GHz)
Architecture: x86â€‘64
RAM: 8â€¯GB total (3â€¯GB free,Â 62â€¯% used)
GPU: IntelÂ UHDÂ GraphicsÂ 630 (0â€¯GB VRAM, integrated)
OS: UbuntuÂ 22.04Â LTSÂ (x64)

ğŸ† Hardware Tier: LOWÂ (OverallÂ Score:Â 45/100)

ğŸ¦™ Ollama Status: âŒ Ollama not runningÂ (connection refused)
```

---

## ğŸ”§ Supported models (40Â +)

### ğŸ£ Ultraâ€‘smallÂ (<Â 1â€¯B params)

- **Qwenâ€¯0.5B** â€“ Ultra lightweight, requiresÂ 1â€¯GB RAM
- **LaMiniâ€‘GPTÂ 774â€¯M** â€“ Multilingual compact model, 1.5â€¯GB RAM

### ğŸ¤ SmallÂ (1â€¯â€“â€¯4â€¯B)

- **TinyLlamaâ€¯1.1â€¯B** â€“ Perfect for testing, 2â€¯GB RAM
- **GemmaÂ 3â€¯1â€¯B** â€“ Mobileâ€‘optimised, 2â€¯GB RAM, 32â€¯K context
- **MobileLLaMAâ€¯1.4â€¯B /Â 2.7â€¯B** â€“ 40â€¯% faster than TinyLlama
- **LlamaÂ 3.2â€¯1â€¯B /Â 3â€¯B** â€“ Compact Meta models
- **Phiâ€‘3Â Miniâ€¯3.8â€¯B** â€“ Great reasoning from Microsoft, 4â€¯GB RAM
- **Gemmaâ€¯2â€¯B** â€“ Efficient Google model, 3â€¯GB RAM

### ğŸ¦ MediumÂ (5â€¯â€“â€¯15â€¯B)

- **LlamaÂ 3.1â€¯8â€¯B** â€“ Perfect balance, 8â€¯GB RAM
- **MistralÂ 7â€¯Bâ€¯v0.3** â€“ Highâ€‘quality EU model, 8â€¯GB RAM
- **QwenÂ 2.5â€¯7â€¯B** â€“ Multilingual with strong coding ability
- **CodeLlamaÂ 7â€¯B** â€“ Specialised for coding, 8â€¯GB RAM
- **DeepSeekÂ CoderÂ 6.7â€¯B** â€“ Advanced code generation
- **Phiâ€‘4Â 14â€¯B** â€“ Latest Microsoft model with improved capabilities
- **GemmaÂ 3â€¯4â€¯B** â€“ Multimodal with long context (128â€¯K)

### ğŸ¦… LargeÂ (>â€¯15â€¯B)

- **LlamaÂ 3.3â€¯70â€¯B** â€“ Meta flagship, 48â€¯GB RAM
- **DeepSeekâ€‘R1â€¯70â€¯B** â€“ Advanced reasoning (o1â€‘style)
- **MistralÂ SmallÂ 3.1â€¯22â€¯B** â€“ Highâ€‘end EU model
- **GemmaÂ 3â€¯12â€¯B /â€¯27â€¯B** â€“ Google multimodal flagships
- **CodeLlamaÂ 34â€¯B** â€“ Heavy coding tasks, 24â€¯GB RAM
- **Mixtralâ€¯8Ã—7â€¯B** â€“ Mixtureâ€‘ofâ€‘Experts, 32â€¯GB RAM

### ğŸ–¼ï¸ MultimodalÂ (VisionÂ +Â text)

- **LLaVAâ€¯7â€¯B** â€“ Image understanding, 10â€¯GB RAM
- **LLaVAâ€‘NeXTâ€¯34â€¯B** â€“ Advanced vision capabilities
- **GemmaÂ 3â€¯4â€¯B /â€¯12â€¯B /â€¯27â€¯B** â€“ Google multimodal family

### ğŸ§² Embedding modelsÂ (semantic search)

- **allâ€‘MiniLMâ€‘L6â€‘v2** â€“ Compact 0.5â€¯GB embedding model
- **BGEâ€‘smallâ€‘enâ€‘v1.5** â€“ Highâ€‘quality English embeddings

### â˜ï¸ Cloud modelsÂ (for comparison)

- **GPTâ€‘4** â€“ OpenAI, requires API key & internet
- **ClaudeÂ 3.5Â Sonnet** â€“ Anthropic, 200â€¯K context

---

## ğŸ› ï¸ Advanced Ollama integration

### Automatic model management

```bash
llm-checker ollama --list       # Details of every local model
llm-checker ollama --running    # Monitor VRAM usage
llm-checker ollama --test llama3.1:8b
```

### Smart installation

```bash
# After analysis, install all recommended models automatically
llm-checker check --filter small | grep "ollama pull" | bash
```

### Realâ€‘time model comparison

```bash
for model in $(ollama list | grep -v NAME | awk '{print $1}'); do
  echo "Testing $model:"
  llm-checker ollama --test $model
done
```

---

## ğŸ“Š Detailed compatibility system

### Scoring scaleÂ (0â€‘100)

| Score | Meaning      |
|-------|--------------|
| 90â€‘100 | ğŸŸ¢ **Excellent** â€“ full speed, all features |
| 75â€‘89  | ğŸŸ¡ **Very good** â€“ great performance |
| 60â€‘74  | ğŸŸ  **Marginal** â€“ usable with tweaks / quantisation |
| 40â€‘59  | ğŸ”´ **Limited** â€“ only for testing |
| 0â€‘39   | âš« **Incompatible** â€“ missing critical hw |

### Compatibility factors

1. **Total RAM vs requirement**Â (40â€¯%)
2. **Available VRAM**Â (25â€¯%)
3. **CPU cores**Â (15â€¯%)
4. **CPU architecture**Â (10â€¯%)
5. **Quantisation availability**Â (10â€¯%)

### Hardware tiers

- ğŸš€ **ULTRA_HIGH** â€“ 64â€¯GBÂ RAM, 32â€¯GBÂ VRAM, 12+â€¯cores
- âš¡ **HIGH** â€“ 32â€¯GBÂ RAM, 16â€¯GBÂ VRAM, 8+â€¯cores
- ğŸ¯ **MEDIUM** â€“ 16â€¯GBÂ RAM,Â 8â€¯GBÂ VRAM, 6+â€¯cores
- ğŸ’» **LOW** â€“ 8â€¯GBÂ RAM,Â 2â€¯GBÂ VRAM, 4+â€¯cores
- ğŸ“± **ULTRA_LOW** â€“ below the above

---

## âš™ï¸ Advanced configuration

### Environment variables

```bash
export OLLAMA_BASE_URL=http://localhost:11434
export LLM_CHECKER_RAM_GB=16
export LLM_CHECKER_VRAM_GB=8
export LLM_CHECKER_CPU_CORES=8
export LLM_CHECKER_LOG_LEVEL=debug
export LLM_CHECKER_CACHE_DIR=/custom/cache
export NO_COLOR=1
```

### Configuration fileÂ `~/.llm-checker.json`

```json
{
  "analysis": {
    "defaultUseCase": "code",
    "performanceTesting": true,
    "includeCloudModels": false
  },
  "ollama": {
    "baseURL": "http://localhost:11434",
    "enabled": true,
    "timeout": 30000
  },
  "display": {
    "maxModelsPerTable": 15,
    "showEmojis": true,
    "compactMode": false
  },
  "filters": {
    "minCompatibilityScore": 70,
    "excludeModels": ["very-large-model"]
  },
  "customModels": [
    {
      "name": "Custom Model 7B",
      "size": "7B",
      "requirements": { "ram": 8, "vram": 4 }
    }
  ]
}
```

---

## ğŸ® Specific use cases

### Coding

```bash
llm-checker check --use-case code --filter medium
ollama pull $(llm-checker check --use-case code --ollama-only | grep "ollama pull" | head -1 | awk '{print $3}')
echo "def fibonacci(n):" | ollama run codellama:7b "Complete this Python function"
```

### Chatbots / assistants

```bash
llm-checker check --use-case chat --filter small,medium
ollama pull llama3.2:3b
ollama run llama3.2:3b "Hello! How can you help me today?"
```

### Semantic search / RAG

```bash
llm-checker check --filter embeddings
ollama pull all-minilm
```

### Multimodal analysis

```bash
llm-checker check --multimodal
ollama pull llava:7b
# echo "image.jpg" | ollama run llava:7b "Describe this image"
```

---

## ğŸ” Troubleshooting

### Ollama not detected

```bash
curl http://localhost:11434/api/version          # Should return JSON

# Install or start service
curl -fsSL https://ollama.ai/install.sh | sh
sudo systemctl start ollama                      # Linux
```

### Incorrect hardware detection

```bash
llm-checker check --detailed
export LLM_CHECKER_RAM_GB=16
export LLM_CHECKER_VRAM_GB=8
llm-checker check
```

### Models marked as incompatible

```bash
llm-checker check --min-score 0
llm-checker check --include-marginal
llm-checker check --quantization Q2_K
```

### Permission errors (Linux/macOS)

```bash
sudo chmod +r /sys/class/dmi/id/*
sudo chmod +r /proc/meminfo
sudo llm-checker check
```

---

## ğŸ§ª Programmatic API (Node)

```javascript
const LLMChecker = require('llm-checker');
const OllamaClient = require('llm-checker/ollama');

const checker = new LLMChecker();
const analysis = await checker.analyze({
  useCase: 'code',
  includeCloud: false,
  performanceTest: true
});

console.log('Compatible models:', analysis.compatible);

const ollama = new OllamaClient();
const localModels = await ollama.getLocalModels();
```

---

## ğŸš€ CI/CD Workflows

### GitHubÂ Actions

```yaml
name: LLM Compatibility Check
on: [push, pull_request]

jobs:
  llm-check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'

    - name: Install LLM Checker
      run: npm install -g llm-checker

    - name: Check LLM Compatibility
      run: |
        llm-checker check --format json > compatibility.json
        cat compatibility.json

    - name: Upload Results
      uses: actions/upload-artifact@v4
      with:
        name: llm-compatibility
        path: compatibility.json
```

### Docker integration

```dockerfile
FROM node:18-alpine

RUN apk add --no-cache dmidecode lm-sensors
RUN npm install -g llm-checker

COPY analyze.sh /usr/local/bin/
RUN chmod +x /usr/local/bin/analyze.sh

CMD ["analyze.sh"]
```

---

## ğŸ¤ Contributing

### Local development

```bash
git clone https://github.com/developer31f/llm-checker.git
cd llm-checker
npm install
npm test
npm run dev check
npm link
```

### Adding new models

Edit `src/models/expanded_database.js`:

```javascript
{
  name: "New Model 7B",
  size: "7B",
  type: "local",
  category: "medium",
  requirements: { ram: 8, vram: 4, cpu_cores: 4, storage: 7 },
  frameworks: ["ollama", "llama.cpp"],
  quantization: ["Q4_0", "Q4_K_M", "Q5_0"],
  performance: {
    speed: "medium",
    quality: "very_good",
    context_length: 8192,
    tokens_per_second_estimate: "15-30"
  },
  installation: {
    ollama: "ollama pull new-model:7b",
    description: "Description of the new model"
  },
  specialization: "general",
  languages: ["en"],
  year: 2024
}
```

### Improving hardware detection

Edit `src/hardware/detector.js`:

```javascript
detectNewGPU(gpu) {
  const model = gpu.model.toLowerCase();
  if (model.includes('new-gpu')) {
    return {
      dedicated: true,
      performance: 'high',
      optimizations: ['new-api']
    };
  }
  return null;
}
```

### Contribution guide

1. **Fork** the repository
2. **Create** a feature branch `git checkout -b feature/awesome-feature`
3. **Commit** your changes `git commit -am 'Add awesome feature'`
4. **Push** the branch `git push origin feature/awesome-feature`
5. **Open** a Pull Request

### Reporting issues

Include the following:

```bash
llm-checker check --detailed --export json > debug-info.json
DEBUG=1 llm-checker check > debug.log 2>&1
uname -a
node --version
npm --version
```

---

## ğŸ“Š Roadmap

### v2.1Â (Q2Â 2025)

- ğŸ”Œ Plugin system for extensions
- ğŸ“± Mobile optimisations
- ğŸŒ Optional web UI
- ğŸ“ˆ Usage metrics & analytics
- ğŸ”„ Automatic model DB updates

### v2.2Â (Q3Â 2025)

- ğŸ¤– More backâ€‘end frameworks (MLX, TensorRT)
- â˜ï¸ Local vs cloud auto comparison
- ğŸ¯ Fineâ€‘tuning advisor
- ğŸ“Š Historical performance dashboard
- ğŸ”’ Enterprise mode

### v3.0Â (Q4Â 2025)

- ğŸ§  AI performance predictor
- ğŸ”„ Multiâ€‘model orchestration
- ğŸ“± Mobile companion app
- ğŸŒ Advanced multilingual models
- ğŸš€ Public cloud integrations

---

## ğŸ† Credits

### Technologies used

- **systeminformation** â€“ crossâ€‘platform HW detection
- **Ollama** â€“ local LLM management
- **Commander.js** â€“ CLI framework
- **Chalk** â€“ terminal colours
- **Ora** â€“ elegant spinners

### Communities & projects

- **llama.cpp** â€“ efficient LLM inference
- **Hugging Face** â€“ model & dataset hub
- **Meta Llama** â€“ openâ€‘source Llama models
- **MistralÂ AI** â€“ European LLMs
- **Google Gemma** â€“ open Gemma family

### Contributors

- **[PavelÂ Chmirenko](mailto:developer31f@gmail.com)** â€“ Lead developer & maintainer


### Special thanks

- **The Ollama team** for an amazing local LLM tool
- **GeorgiÂ Gerganov** for `llama.cpp`
- **The openâ€‘source community** for making AI accessible

---

## ğŸ“„ License

MIT â€“ see [LICENSE](LICENSE) for details.

---

## ğŸ’ Support the project

If **LLM Checker** saves you time:

â­ **Star** the repo  
ğŸ› **Report bugs** & suggest features  
ğŸ¤ **Contribute** code or docs  
ğŸ“¢ **Share** with fellow devs  
â˜• **[Buy me a coffee](https://buymeacoffee.com/pavelchmirenko)**

---

<div align="center">

**Got questions?** ğŸ’¬ [Open an issue](https://github.com/developer31f/llm-checker/issues)  
**Want to contribute?** ğŸš€ [Read the guide](CONTRIBUTING.md)  
**Need advanced usage?** ğŸ“š [Full docs](ADVANCED_USAGE.md)

**Made with â¤ï¸ for the openâ€‘source AI community**

</div>
