{
  "name": "llm-checker",
  "version": "3.2.3",
  "description": "Intelligent CLI tool with AI-powered model selection that analyzes your hardware and recommends optimal LLM models for your system",
  "bin": {
    "llm-checker": "bin/cli.js",
    "ollama-checker": "bin/cli.js",
    "llm-checker-mcp": "bin/mcp-server.mjs"
  },
  "main": "src/index.js",
  "scripts": {
    "test": "node tests/run-all-tests.js",
    "test:gpu": "node tests/gpu-detection/multi-gpu.test.js",
    "test:platform": "node tests/platform-tests/cross-platform.test.js",
    "test:ui": "node tests/ui-tests/interface.test.js",
    "test:runtime": "node tests/runtime-specdec-tests.js",
    "test:deterministic-pool": "node tests/deterministic-model-pool-check.js",
    "test:policy": "node tests/policy-commands.test.js",
    "test:policy-cli": "node tests/policy-cli-enforcement.js",
    "test:policy-engine": "node tests/policy-engine.test.js",
    "test:policy-e2e": "node tests/policy-e2e-integration.test.js",
    "test:hardware-detector": "node tests/hardware-detector-regression.js",
    "test:all": "node tests/run-all-tests.js",
    "build": "echo 'No build needed'",
    "dev": "node bin/enhanced_cli.js",
    "start": "node bin/enhanced_cli.js",
    "check": "node bin/enhanced_cli.js check",
    "recommend": "node bin/enhanced_cli.js recommend",
    "ollama": "node bin/enhanced_cli.js ollama",
    "list-models": "node bin/enhanced_cli.js list-models",
    "ai-check": "node bin/enhanced_cli.js ai-check",
    "ai-run": "node bin/enhanced_cli.js ai-run",
    "benchmark": "cd ml-model && python python/benchmark_collector.py",
    "train-ai": "cd ml-model && python python/train_model.py",
    "postinstall": "echo 'LLM Checker installed. Run: llm-checker hw-detect'"
  },
  "dependencies": {
    "@modelcontextprotocol/sdk": "^1.26.0",
    "chalk": "^4.1.2",
    "commander": "^11.1.0",
    "inquirer": "^8.2.6",
    "node-fetch": "^2.7.0",
    "ora": "^5.4.1",
    "systeminformation": "^5.21.0",
    "table": "^6.8.1",
    "yaml": "^2.8.1",
    "zod": "^3.23.0"
  },
  "optionalDependencies": {
    "sql.js": "^1.14.0"
  },
  "devDependencies": {
    "@types/node": "^20.0.0",
    "jest": "^29.7.0"
  },
  "keywords": [
    "llm",
    "ai",
    "intelligent-selector",
    "model-recommendation",
    "hardware-analysis",
    "machine-learning",
    "hardware",
    "compatibility",
    "cli",
    "ollama",
    "small-language-models",
    "sllm",
    "local-ai",
    "quantization",
    "inference",
    "gpu",
    "vram",
    "performance",
    "benchmark",
    "apple-silicon",
    "memory-optimization",
    "mcp",
    "claude",
    "model-context-protocol"
  ],
  "author": "Pavelevich (https://github.com/Pavelevich)",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/Pavelevich/llm-checker.git"
  },
  "homepage": "https://github.com/Pavelevich/llm-checker#readme",
  "bugs": {
    "url": "https://github.com/Pavelevich/llm-checker/issues"
  },
  "readmeFilename": "README.md",
  "engines": {
    "node": ">=16.0.0",
    "npm": ">=8.0.0"
  },
  "os": [
    "darwin",
    "linux",
    "win32"
  ],
  "preferGlobal": true,
  "files": [
    "bin/",
    "src/",
    "analyzer/",
    "README.md",
    "LICENSE"
  ]
}
